import pandas as pd
import warnings
import os
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedShuffleSplit
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform, uniform
from sklearn.metrics import accuracy_score, roc_auc_score
import time, datetime
from sklearn.linear_model import LogisticRegression

warnings.filterwarnings('ignore') # This command suppresses all warnings.

cwd = os.getcwd() # Get Current Working Directory.

print(cwd) # Print Current Working Directory. 

df = pd.read_csv('winequality-white.csv' ,sep =';') # Import a csv file.

print(df.head()) # Viewing the first 5 lines.

print(df.tail()) # Viewing the last 5 lines.

print(df.isnull().sum()) # Detect missing values(NAN in numeric arrays).

print(df.dtypes) # Check data type of each column.

sns.boxplot(y=df["alcohol"]) # Boxplot.

plt.show() # Display the graph.

print(f"min_alcohol: {min(df["alcohol"])}", f"max_alcohol: {max(df["alcohol"])}", sep="\n") # Print min,max of the variable alcohol.

for i in range(len(df.columns)): # Display the descriptive statistics of each variable separately.
    print(df.iloc[:,i].describe().round(3))
    
df = df[(np.abs(stats.zscore(df) < 3).all(axis = 1))] # Remove outliers using the Z-score technique.

X = df.drop(["quality"], axis = 1) # Create a new dataframe without the quality column.

y = df["quality"] # Creating a vector using the quality column.

print(X.shape) # Check the dimensions.

Scaler = MinMaxScaler(feature_range=(0, 1)) # Scale each feature to a range between 0 and 1

Scaler.fit(X) # Compute minimum and maximum per feature. 

X = Scaler.fit_transform(X) # Transform the data(values between 0 and 1).

X = pd.DataFrame(X, columns = ["fixed acidity", "volatile acidity", "citric acid", "residual sugar",
                 "chlorides", "free sulfur dioxide", "total sulfur dioxide", "density", "pH",
                 "sulphates", "alcohol"]) # Convert X back to dataframe.

corr = X.corr() # Computes the correlation between the variables.

ax = sns.heatmap(corr, annot = True, fmt = ".2f", cmap = "cividis") # It constructs a heatmap using the dataframe corr, which contains the linear correlation between the variables.  

ax.xaxis.tick_top() # Sets the position of the X-axis labels.

ax.set_xticklabels(ax.get_xticklabels(), rotation = 25) # # Sets X-axis labels.

plt.show() # Display the graph.

print(X.head()) # For checking purposes.

# It is noteworthy that several of the variables do not exhibit a strong linear correlation, which can potentially impact the PCA process.
# This is because PCA attempts to reduce data dimensionality while retaining as much variance as possible.
# The new features generated by PCA to reduce the variables are formed through a linear transformation.
# More specifically, each component is a linear weighted combination of the original variables.
# In conclusion, we find that the greater the correlation among the variables, the fewer components will be needed to explain the desired proportion of variance.
# The strength of the statistical technique known as PCA lies in its ability to reduce the dimensionality of datasets characterized by complex relationships,
# while effectively retaining the variance. Another positive aspect is that the components are uncorrelated with each other.
# One of the reasons we use PCA is to select factors that will later be used in the machine learning phase.
# It also reduces the number of input variables used in the subsequent model.
# Finally, a data set with many variables can lead to overfitting issues and additional computational costs, which we want to avoid at all costs.

pca = PCA(n_components = 6) # Principal component analysis (PCA), We retain 6 variables while preserving as much variance as possible.

pca.fit(X) # PCA is applied to our dataset.

print(pca.components_) # Eigenvectors.

print(pca.explained_variance_) # Eigenvalues.

print(pca.explained_variance_ratio_.round(2)) # The percentage of variance explained by each selected component(linear combination of the original variables).

pc = pca.fit_transform(X) # It returns a type of array where each variable is now a component which is a linear permutation of the initial variables.

pc_X = pd.DataFrame(data = pc, columns = ["PC_1", "PC_2", "PC_3", "PC_4", "PC_5", "PC_6"]) # Conversion to Dataframe and definition of names in the columns

print(pc_X.head()) # Checking.

print(pc_X.shape) # Checking.

sss = StratifiedShuffleSplit(n_splits = 5, test_size = 0.20, random_state = 0) # It splits the data into train/test sets using indices,
                                                                              # while maintaining the proportion of each class in each split(subset).
                                                                              
split_indices = sss.split(pc_X, y) # It will generate indices to split the dataset into training/test sets(The ratio in classes is maintained in each subset based on the y variable).

for train_index, test_index in split_indices: # The data is split into subsets using the indices. 
    X_remainings, X_validation = pc_X.iloc[train_index], pc_X.iloc[test_index]
    y_remainings, y_validation = y.iloc[train_index], y.iloc[test_index]

print(X_remainings.head(), X_remainings.shape, y_remainings.head(), y_remainings.shape, sep="\n") # Checking.

SSS = StratifiedShuffleSplit(n_splits = 5, test_size = 0.30, random_state = 0) # This is done to split the remaining dataset, since some of it will be used for the validation set,
                                                                              # into training and test sets while maintaining similar proportions of values
                                                                              # for the variable y within the subsets.

for train_index, test_index in SSS.split(X_remainings, y_remainings):
    X_train, X_test = X_remainings.iloc[train_index], X_remainings.iloc[test_index]
    y_train, y_test = y_remainings.iloc[train_index], y_remainings.iloc[test_index]

print(X_train.head(), X_train.shape, y_train.head(), y_train.shape, sep = "\n") # Checking.

# Next, we will use the LightGBM algorithm to predict the target variable, which is wine quality.
# LightGBM is an algorithm that is based on gradient boosting and relies on trees for its learning process.
# It is an algorithm that is fast in execution(training process), efficient in resource usage, and accurate in its performance across a wide range of applications.
# It is worth mentioning that it is more efficient and faster than the XGBoost algorithm.
# A key difference with this specific algorithm compared to other decision tree learning algorithms is that the tree growth is leaf-based.
# This means it selects the leaf that achieves the best optimization of the objective function and continues developing the tree from that node.
# One of the techniques used by this specific algorithm is called Gradient-based One-Side Sampling (GOSS). This technique reduces the number of data instances.
# It keeps only the instances with high training error and randomly selects a portion of the instances
# with low error to maintain the data distribution. This ensures that the performance of the model is
# not affected, as the distribution impacts the different conditions a variable may encounter during
# training. At the same time, it places more emphasis on the instances with high error.
# It also employs a technique aimed at effectively reducing the features and thus decreasing the complexity during the training of the model.
# This is achieved by converting many features into one, called an exclusive feature bundle.
# They do this by merging variables that cannot take the same value at the same time into a single feature containing discrete bins.
# Discrete bins represent ranges of numbers, where multiple values are grouped and represented by fewer values. Essentially, many values are simplified into fewer categories.

LGBM = lgb.LGBMClassifier(random_state = 0, data_sample_strategy = 'goss', verbose = -1) # lightGBM for categorical target variable.

print(LGBM.get_params()) # It will display the parameters of the specific algorithm (LightGBM).

param = { 'learning_rate' : loguniform.rvs(0.0001, 0.1, size = 150, random_state = 0), # It adjusts the contribution of each weak learner in the model's learning process.
                                                             # Regarding the learning rate variable, parameters will be randomly selected from the logistic distribution.  
          'max_depth' : [1, 2, 3], # It defines the maximum depth that the trees (weak learners) will have(handling overfitting).
          'n_estimators' : [100, 200, 300, 400, 500, 600, 700], # The number of weak learners that will be used in the boosting process.
          'num_leaves' : [1, 2, 3, 4, 5], # It defines the maximum number of leaves that weak learners can have. 
          'min_child_samples': [20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150], # Minimum number of observations that each leaf, which resulted from a split, must have. 
          'subsample' : uniform.rvs(0.1, 1, size = 150, random_state = 0) } # Sets the percentage of the sample that will be picked randomly for the training of weak learners.
                                  # This specific parameter will be selected from a random variable, where each value in the interval has an equal probability of being chosen. 

# In contrast to the popular GridSearchCV, which tests all possible combinations of parameters,
# RandomizedSearchCV randomly selects some of the possible parameter values with equal probability.
# This methodology explores the hyperparameter space, effectively tuning the algorithm's parameters with the aim of improving its performance.
# This significantly improves the computational efficiency of the algorithm and provides a more effective approach to parameter optimization.
# This specific hyperparameter tuner has "CV" at the end, which corresponds to the word Cross-validation.
# In our case, cross-validation will split the validation set into k equal, non-overlapping subsets.
# The model will be trained on k-1 subsets in each iteration and evaluated on the remaining subset.
# This approach provides a more reliable estimation of the model's performance and allows for a better selection of hyperparameters.

Random_Search = RandomizedSearchCV(estimator = LGBM, param_distributions = param, n_iter = 150, scoring = 'accuracy', n_jobs = -1, random_state = 0) # RandomizedSearchCV.

start = time.time() # It gives the time elapsed from a predefined date in seconds.
                    # We do this to measure how much time is required for the optimization of hyperparameters and the training process of the model.

Random_Search.fit(X_validation, y_validation) # All hyperparameter sets selected by the optimization technique will be tested on the validation set. 

print(Random_Search.best_params_) # It displays the set of parameters that gave the best result on the validation set.

LGBM = LGBM.set_params(**Random_Search.best_params_) # The estimated set of hyperparameters that gave the best result will be assigned to the corresponding parameters of the model.

LGBM.fit(X_train, y_train) # It builds a gradient boosting model using the training set, appropriately estimating the model's parameters with the goal of minimizing the cost function.

print(f'Time in h m s format: {datetime.timedelta(seconds = time.time() - start)}') # Convert seconds to hours minutes seconds format.

y_pred_train = LGBM.predict(X_train) # Using the input features, the target variable y will be predicted.

y_pred_proba_train = LGBM.predict_proba(X_train) # Prediction of the probability of occurrence for each category according to the sample(Train set).

y_pred_test = LGBM.predict(X_test) # Using the instances from input features from the test set (unseen data), the model will predict the target variable y.

y_pred_proba_test = LGBM.predict_proba(X_test) # Prediction of the probability of occurrence for each category according to the sample(Test set).

# The first evaluation metric we selected for our model is the accuracy_score,
# which corresponds to the percentage of values correctly predicted by the model relative to the entire sample.
# We also use the ROC AUC metric for a better understanding of the model's performance.
# It is a curve where each point is represented by two values, the probability that an observation is classified
# as positive and is actually positive, known as the true positive rate, and the probability that an
# observation is classified as positive but is actually negative, known as the false positive rate.
# A ROC value of 0.5 corresponds to random guessing, meaning there is a 50% chance of correctly classifying a positive or negative observation.
# If ROC = 1, it means the model has a 100% probability of correctly classifying a positive instance and a 0% probability
# of misclassifying a negative instance as positive(no false negatives).
# We observe that the metric roc_auc_score has a parameter multi_class = 'ovr'(ovr = One-vs-rest).
# It calculates the ROC curve for each combination where one option is a specific category and the other option includes the remaining categories,
# with the classifier's goal being to distinguish the given category from the others. 

print(f'The training error of LightGBM(accuracy): {accuracy_score(y_train, y_pred_train):.3f}', # Display the  accuracy, ROC AUC score(classification) of the model(LightGBM)
      f'The generalization error of LightGBM(accuracy): {accuracy_score(y_test, y_pred_test):.3f}', # on the train and test sets.
      f'The training error of LightGBM(ROC AUC): {roc_auc_score(y_train, y_pred_proba_train, average = 'weighted', multi_class = 'ovr'):.3f}', 
      f'The Generalization error of LightGBM(ROC AUC): {roc_auc_score(y_test, y_pred_proba_test, average = 'weighted', multi_class = 'ovr'):.3f}',
      sep ='\n')

# Logistic regression is used in classification problems, and we are talking about a quite famous machine learning method with tremendous success,
# which we will use as a benchmark model. In the binary case, it calculates the probability that the variable y takes the value 1 according to the variables X,
# by fitting a logistic S-shaped function, assuming that the variable y takes values between 0 and 1. Using a threshold value, for example, if the probability is >50%,
# the variable y belongs to the category with a value of one, then it will be classified as y = 1.   
# In our case, based on the independent variables, it estimates the probability for each category of the dependent variable y in such a way that all together sum up to 1.
# This is called multinomial logistic regression, and it involves more than one logistic function, where one category is used as the baseline to calculate the logistic probabilities,
# as is also the case in the simple binary scenario. Each function, using the independent variables X,
# calculates the probability of each respective category, considering that all probabilities together must sum up to 1.
# The coefficients β indicate how each variable X affects the logarithmic odds of each respective category.

LR = LogisticRegression(random_state = 0) # Implementation of Logistic Regression(classifier).

LR.fit(X_train, y_train) # The model is fitted to the training set by estimating the coefficients α and β. 

y_pred_train_LR = LR.predict(X_train) # Prediction of the target variable y given the variables X(train set). 

y_pred_test_LR = LR.predict(X_test) # Prediction of the target variable y given the variables X(test set).

print('The training error of Logistic Regression(accuracy): %.3f'% accuracy_score(y_train, y_pred_train_LR),
      'The generalization error of Logistic Regression(accuracy): %.3f'% accuracy_score(y_test, y_pred_test_LR),
      sep = '\n') # Display the  accuracy of the model(Logistic Regression) on the train and test sets.

# We observe that regarding the performance of the two models, the benchmark model performs very close to our model.
# This means that the seemingly simple model, with a significantly faster execution time and without hyperparameter optimization, achieved similar performance to the LightGBM model.
# Therefore, we conclude that the more complex model did not provide any better predictive capability compared to the simpler model.
# This could be due either to the preprocessing of the data or to the data itself.
